{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## IMPORTS ##########\n",
    "\n",
    "# common\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# shapefile to mask for stats\n",
    "import geopandas as gpd\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "# stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Haralick textures quick computation\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## DATA ##########\n",
    "\n",
    "list_raster_pairs = [(r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_pre.tif\",\n",
    "                       r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_post.tif\")]\n",
    "shp_path = r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_grd_truth\\zta1_grd_truth.shp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## DERIVED INDICES  ##########\n",
    "\n",
    "def get_sar_indices_and_profile(input_path: str, output_path: str = None ) -> tuple[dict, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates SAR vegetation and degradation indices from Gamma0 and Coherence bands.\n",
    "    \n",
    "    Input band order (expected):\n",
    "    1: Gamma0 VH\n",
    "    2: Gamma0 VV\n",
    "    3: Coherence VH\n",
    "    4: Coherence VV\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (updated_profile, output_stack)\n",
    "    \"\"\"\n",
    "    with rasterio.open(input_path) as src:\n",
    "        # Load input bands\n",
    "        gamma_vh = src.read(1).astype('float32')\n",
    "        gamma_vv = src.read(2).astype('float32')\n",
    "        coh_vh = src.read(3).astype('float32')\n",
    "        coh_vv = src.read(4).astype('float32')\n",
    "        \n",
    "        # Copy and prepare the profile for 9 bands\n",
    "        profile = src.profile.copy()\n",
    "        \n",
    "        # Set error handling for division by zero (common in radar shadows) and invalid divisions (e.g. 0/0, inf-inf)\n",
    "        np.seterr(divide='ignore', invalid='ignore')  # remove warnings linked problematic divisions, sets division by 0 to +-inf and invalid divisions to NaN \n",
    "\n",
    "        # --- Gamma0 Derived Indices ---\n",
    "        # Back-scattering ratio: q = VH / VV\n",
    "        q_ratio = gamma_vh / gamma_vv\n",
    "        \n",
    "        # Radar Vegetation Index: RVI = (4 * VH) / (VV + VH)\n",
    "        rvi = (4 * gamma_vh) / (gamma_vv + gamma_vh)\n",
    "        \n",
    "        # Dual-polarization SAR vegetation index: DPSVI = VV / (VV + VH)\n",
    "        dpsvi = gamma_vv / (gamma_vv + gamma_vh)\n",
    "        \n",
    "        # Radar forest degradation index: RFDI = (VV - VH) / (VV + VH)\n",
    "        rfdi = (gamma_vv - gamma_vh) / (gamma_vv + gamma_vh)\n",
    "\n",
    "        # --- Coherence Derived Index ---\n",
    "        # Coherence ratio: Cq = VH / VV\n",
    "        cq_ratio = coh_vh / coh_vv\n",
    "\n",
    "        # Clean up calculation artifacts (set inf and -inf to nan, keep nan as nan)\n",
    "        # We include input bands in the cleanup to ensure the whole stack is consistent\n",
    "        all_bands = [gamma_vh, gamma_vv, q_ratio, rvi, dpsvi, rfdi, coh_vh, coh_vv, cq_ratio]\n",
    "        \n",
    "        for layer in all_bands:\n",
    "            # np.isfinite returns False for NaN, Inf and -Inf\n",
    "            layer[~np.isfinite(layer)] = np.nan\n",
    "\n",
    "        # --- Stack the final 9 bands in one object of size (9, H, W)---\n",
    "        output_stack = np.stack(all_bands)\n",
    "\n",
    "        # Update metadata to reflect the 9-band output\n",
    "        profile.update({\n",
    "            \"count\": 9,\n",
    "            \"dtype\": 'float32',\n",
    "            \"nodata\": np.nan  \n",
    "        })\n",
    "    \n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(output_stack)\n",
    "            dst.descriptions = ('G0_VH', 'G0_VV', 'q_ratio', 'RVI', 'DPSVI', 'RFDI', 'coh_VH', 'coh_VV', 'cq_ratio')\n",
    "\n",
    "        print(f\"Success! Indices saved to: {output_path}\")\n",
    "\n",
    "        return profile, output_stack\n",
    "    \n",
    "# Example \n",
    "\n",
    "pre_path, post_path = r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_explore\\zta1_pre.tif\", r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_explore\\zta1_post.tif\"\n",
    "output_path_pre = r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_explore\\zta1_pre_indices.tif\"\n",
    "output_path_post = r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_explore\\zta1_post_indices.tif\"\n",
    "\n",
    "get_sar_indices_and_profile(pre_path, output_path_pre)\n",
    "get_sar_indices_and_profile(post_path, output_path_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## TEXTURES ##########\n",
    "\n",
    "\n",
    "def _compute_row_glcm_features(y: int, padded_img: np.ndarray, window_size: int, mask_row: np.ndarray, levels: int = 32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes 9 GLCM textures for a single row.\n",
    "    Standard properties ('contrast', 'dissimilarity', 'homogeneity', 'correlation', 'energy', 'ASM')\n",
    "    from skimage  + Variance + Entropy + Mean.\n",
    "    \"\"\"\n",
    "    width = mask_row.shape[0]\n",
    "    # Standard properties from skimage\n",
    "    properties = ['contrast', 'dissimilarity', 'homogeneity', 'correlation', 'energy', 'ASM']\n",
    "    n_features = 9 # 6 standard + 3 custom\n",
    "    row_out = np.full((n_features, width), np.nan, dtype='float32') # creates a nan ouput, where the valid pixels will be filled\n",
    "    \n",
    "    # Pre-calculate gray level indices for Variance/Entropy (0 to levels-1)\n",
    "    i_indices = np.arange(levels).reshape(levels, 1, 1, 1) \n",
    "    \n",
    "    for x in range(width):\n",
    "        if mask_row[x]: # not a valid pîxel \n",
    "            continue\n",
    "            \n",
    "        window = padded_img[y : y + window_size, x : x + window_size]\n",
    "        \n",
    "        # 1. Compute GLCM (levels, 4 directions) for one pixel \n",
    "        # The GLCM is a 4D array with shape: (levels, levels, n_distances, n_angles).\n",
    "        # - levels: The number of gray levels (e.g., 32). The first two dimensions (rows/cols) \n",
    "        #   represent the probability P(i,j) (relative frequency here) of moving from gray level i to j.\n",
    "        # - n_distances: The number of spatial offsets used (usually 1).\n",
    "        # - n_angles: The number of directions analyzed (e.g., 0°, 45°, 90°, 135°).\n",
    "        # Each element glcm[i, j, d, a] represents the probability of a pixel with intensity i having a neighbor with intensity j at distance d and angle a.\n",
    "        glcm = graycomatrix(window, distances=[1], \n",
    "                            angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], \n",
    "                            levels=levels, symmetric=True, normed=True)\n",
    "        \n",
    "        # 2. Extract standard skimage properties\n",
    "        for i, prop in enumerate(properties):\n",
    "            row_out[i, x] = np.mean(graycoprops(glcm, prop)) # mean because four directions in the graycomatrix, we want to detect texture whatever the direction of the shapes\n",
    "            \n",
    "        # 3. Manual calculation: Variance (Sum of Squares)\n",
    "        # It is the weighted sum of gray levels (i) by their probabilities (=frequencies here) P(i,j).\n",
    "        # axis=(0, 1) sums over rows (i) and columns (j) of the matrix.\n",
    "        # Variance measures the dispersion of gray levels around the mean.\n",
    "        # Formula: Sum over i,j of P(i,j) * (i - mu)^2.\n",
    "        # i_indices - mu uses broadcasting to subtract mu from each gray level i.\n",
    "        # We use broadcasting to perform calculations across all angles simultaneously without loops:\n",
    "        # - glcm has shape (32, 32, 1, 4)\n",
    "        # - i_indices has shape (32, 1, 1, 1)\n",
    "        # When multiplying (i_indices * glcm), NumPy automatically \"stretches\" the 1s in i_indices \n",
    "        # to match the shape of glcm, allowing for ELEMENT-WISE multiplication across all dimensions.\n",
    "\n",
    "        mu = np.sum(i_indices * glcm, axis=(0, 1))\n",
    "        var = np.sum(glcm * (i_indices - mu)**2, axis=(0, 1))\n",
    "        row_out[6, x] = np.mean(var) # mean to aggregate the directions\n",
    "        row_out[7, x] = np.mean(mu) # idem\n",
    "        \n",
    "        # 4. Manual calculation: Entropy -x*lnx\n",
    "        ent = -np.sum(glcm * np.log(glcm + 1e-10), axis=(0, 1))  # 1e-10 to avoid log(0)\n",
    "        row_out[8, x] = np.mean(ent) # mean to aggregate the directions\n",
    "            \n",
    "    return row_out\n",
    "\n",
    "def process_raster_textures(input_path: str, output_path: str, window_size: int = 7, levels: int = 32,\n",
    "                            target_band_indices: list = None, custom_labels: list = None):\n",
    "    \"\"\"\n",
    "    Computes GLCM textures for selected or all bands.\n",
    "    Input: Multi-band raster.\n",
    "    Output: Multi-band raster containing [Original Bands] + [Generated Textures].\n",
    "    \"\"\"\n",
    "    with rasterio.open(input_path) as src:\n",
    "        original_data = src.read().astype('float32')\n",
    "        profile = src.profile.copy()\n",
    "        # Use existing descriptions, custom labels, or generic names\n",
    "        input_names = custom_labels if custom_labels else (list(src.descriptions) if src.descriptions[0] else [f\"Band_{i+1}\" for i in range(src.count)])\n",
    "\n",
    "    n_bands, h, w = original_data.shape\n",
    "    pad = window_size // 2\n",
    "    tex_names = ['contrast', 'dissimilarity', 'homogeneity', 'correlation', 'energy', 'ASM', 'variance', 'mean', 'entropy']\n",
    "    \n",
    "    # Define which bands to process (default: all)\n",
    "    indices_to_process = target_band_indices if target_band_indices is not None else list(range(n_bands))\n",
    "    \n",
    "    all_texture_results = []\n",
    "    output_labels = list(input_names) # Start labels with original band names\n",
    "\n",
    "    for b_idx in indices_to_process:\n",
    "        band_data = original_data[b_idx]\n",
    "        mask = np.isnan(band_data)\n",
    "        band_name = input_names[b_idx]\n",
    "        \n",
    "        print(f\"Processing textures for: {band_name}...\")\n",
    "\n",
    "        # 1. Quantization (0-(levels-1))  \n",
    "        if np.any(~mask):\n",
    "            b_min, b_max = np.nanmin(band_data), np.nanmax(band_data)\n",
    "            clean_band = np.nan_to_num(band_data, nan=b_min) # can't process textures on NaN\n",
    "            # Normalization per band to handle different signal ranges (e.g., VV vs VH)\n",
    "            band_32 = ((clean_band - b_min) / (b_max - b_min + 1e-6) * (levels-1)).astype(np.uint8)\n",
    "        else:\n",
    "            print(f\"Skipping empty band {b_idx}\")\n",
    "            continue\n",
    "\n",
    "        padded_band = np.pad(band_32, pad, mode='reflect') # to handle the sliding window when on the side of the raster\n",
    "\n",
    "        # 2. Parallel row processing for this specific band\n",
    "        results = Parallel(n_jobs=-1)(\n",
    "            delayed(_compute_row_glcm_features)(y, padded_band, window_size, mask[y]) \n",
    "            for y in range(h)\n",
    "        )\n",
    "\n",
    "        # 3. Reconstruct (9, H, W) from results and store\n",
    "        band_textures = np.transpose(np.array(results), (1, 0, 2))\n",
    "        all_texture_results.append(band_textures)\n",
    "        \n",
    "        # 4. Generate labels for these textures\n",
    "        output_labels.extend([f\"{band_name}_{t}\" for t in tex_names])\n",
    "\n",
    "    # Final stack: [Original Bands] + [All Generated Textures]\n",
    "    final_output = np.concatenate([original_data] + all_texture_results, axis=0)\n",
    "\n",
    "    # 5. Metadata Update\n",
    "    profile.update(count=final_output.shape[0], dtype='float32', nodata=np.nan)\n",
    "\n",
    "    with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "        dst.write(final_output)\n",
    "        dst.descriptions = tuple(output_labels)\n",
    "    \n",
    "    print(f\"Success! Generated {final_output.shape[0]} bands in: {output_path}\")\n",
    "    return final_output, output_labels\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# ['G0_VH', 'G0_VV', 'q_ratio', 'RVI', 'DPSVI', 'RFDI', 'coh_VH', 'coh_VV', 'cq_ratio']\n",
    "pre_path, post_path = r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_pre.tif\", r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_post.tif\"\n",
    "output_path = r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\explore\\zta1_post_40.tif\"\n",
    "process_raster_textures(post_path, output_path, target_band_indices=[0, 1, 2, 3], custom_labels=['G0_VH', 'G0_VV','coh_VH','coh_VV']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## VARIABLES CORRELATION ##########\n",
    "\n",
    "\n",
    "def compute_mean_difference_correlation_study(list_raster_pairs: list[tuple[str, str]]) -> np.ndarray :\n",
    "    \"\"\"\n",
    "    Computes the absolute difference for each pair of n-band rasters, \n",
    "    calculates the correlation matrix for each, and returns the average correlation matrix.\n",
    "    \"\"\"\n",
    "    all_corr_matrices = []\n",
    "    n_bands = None\n",
    "\n",
    "    for (path1, path2) in list_raster_pairs:\n",
    "        with rasterio.open(path1) as src1, rasterio.open(path2) as src2:\n",
    "            # Check if band counts match\n",
    "            if src1.count != src2.count:\n",
    "                print(f\"[SKIP] Band mismatch: {path1} and {path2}\")\n",
    "\n",
    "                continue\n",
    "            \n",
    "            if n_bands is None:\n",
    "                n_bands = src1.count\n",
    "            \n",
    "            # Read all bands: shape (n_bands, height, width)\n",
    "            img1 = src1.read().astype(np.float32)\n",
    "            img2 = src2.read().astype(np.float32)\n",
    "\n",
    "            # 1) Difference\n",
    "            diff = img1 - img2\n",
    "\n",
    "            # 2) Reshape for correlation: (n_bands, total_pixels)\n",
    "            # We flatten height and width into a single dimension\n",
    "            diff_flattened = diff.reshape(n_bands, -1)  # -1 asks reshape tu automatically calculate the size of that dimension so as to keep the same number of total element \n",
    "\n",
    "            # 3) Identify valid pixels (where no band has a NaN)\n",
    "            valid_mask = ~np.isnan(diff_flattened).any(axis=0)  # first part eliminates pixels that are NaN on at least a band (such a pixel = False in the mask)\n",
    "\n",
    "            # Filter to keep only valid pixels\n",
    "            diff_cleaned = diff_flattened[:, valid_mask]\n",
    "            \n",
    "            # 4) Calculate correlation matrix (n_bands x n_bands)\n",
    "            # np.corrcoef treats each row as a variable (band)\n",
    "            if diff_cleaned.shape[1] > 1:\n",
    "\n",
    "                corr_matrix = np.corrcoef(diff_cleaned)\n",
    "\n",
    "                # Handle potential NaN (e.g. if a band has zero variance in the difference (0/0))\n",
    "                corr_matrix = np.nan_to_num(corr_matrix)   # will put a 0 in case of a  0/0 (no correlation in this case)\n",
    "                all_corr_matrices.append(corr_matrix)\n",
    "            else:\n",
    "                # Notification if the pair is empty\n",
    "                print(f\"[WARNING] less than 2 (1 or 0) valid pixels found for pair: {path1} and {path2}. Skipping.\")\n",
    "\n",
    "    if not all_corr_matrices:  # list empty\n",
    "        raise ValueError(\"No valid correlation matrices were computed.\")\n",
    "\n",
    "    # 4) Compute the mean correlation matrix\n",
    "    mean_corr_matrix = np.mean(np.abs(all_corr_matrices), axis=0) # axis=0 because mean of the matrices (all_corr_matrices is a 3D block)\n",
    "\n",
    "    return mean_corr_matrix\n",
    "\n",
    "def display_correlation_matrix(matrix: np.ndarray, labels: list[str] = None):\n",
    "    \"\"\"\n",
    "    Simple display utility for the correlation matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 12))\n",
    "    im = ax.imshow(matrix, cmap=\"YlOrRd\", vmin= 0, vmax=1)\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    if labels:\n",
    "        ax.set_xticks(np.arange(len(labels)))\n",
    "        ax.set_yticks(np.arange(len(labels)))\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set_yticklabels(labels)\n",
    "\n",
    "    # --- Add numerical values inside the matrix ---\n",
    "    # We iterate over rows (i) and columns (j)\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            # Display the value formatted to 2 decimal places\n",
    "            # ha/va are for horizontal and vertical alignment\n",
    "            val = matrix[i, j]\n",
    "            color = \"white\" if abs(val) > 0.7 else \"black\" # Adjust text color for contrast\n",
    "            ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=color, fontsize=8)\n",
    "\n",
    "    ax.set_title(\"Mean Correlation Matrix of Band Differences\")\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "\n",
    "list_raster_pairs = [(r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_explore\\zta1_pre_indices.tif\", r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_explore\\zta1_post_indices.tif\")]\n",
    "mean_corr = compute_mean_difference_correlation_study(list_raster_pairs)\n",
    "display_correlation_matrix(mean_corr, labels=[f\"{i+1}\" for i in range(mean_corr.shape[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CPA ##########\n",
    "\n",
    "\n",
    "def perform_global_pca_analysis(list_raster_pairs, list_shp_paths, n_components=2, labels: list = None):\n",
    "    \"\"\"\n",
    "    Performs Global PCA and correlation circle visualizatioon on the (pre-post) difference of multiple (pre, post) pairs using specific shapefiles for each pair.\n",
    "    Calculates the mean position of each class.\n",
    "    \"\"\"\n",
    "    all_pixels_list = [] \n",
    "    all_labels_list = [] \n",
    "\n",
    "    for i, ((path1, path2), shp_path) in enumerate(zip(list_raster_pairs, list_shp_paths)):\n",
    "        with rasterio.open(path1) as src1, rasterio.open(path2) as src2:\n",
    "            \n",
    "            # 1) CHECK PROFILE CONSISTENCY\n",
    "            # We compare the metadata\n",
    "            profiles_match = {k: v for k, v in src1.profile.items() if k != 'nodata'} == {k: v for k, v in src2.profile.items() if k != 'nodata'}\n",
    "            if not profiles_match: # if we compare nodata, np.nan == np.nan systematically returns false \n",
    "                print(f\"[WARNING] Profile mismatch in pair {i+1}:\")\n",
    "                print(f\"  - Pre:  {src1.profile}\")\n",
    "                print(f\"  - Post: {src2.profile}\")\n",
    "                # You might want to raise an error or continue with caution\n",
    "            \n",
    "            # Read images and compute difference\n",
    "            n_bands = src1.count\n",
    "            diff = (src1.read().astype(np.float32) - src2.read().astype(np.float32))\n",
    "            diff_flat = diff.reshape(n_bands, -1).T \n",
    "\n",
    "            # Rasterize Ground Truth\n",
    "            gdf = gpd.read_file(shp_path)\n",
    "            \n",
    "            if gdf.crs != src1.crs: # Adding a clear notification about the CRS mismatch\n",
    "                print(f\"[INFO] CRS mismatch for {shp_path}. Reprojecting from {gdf.crs} to {src1.crs}\")\n",
    "                gdf = gdf.to_crs(src1.crs)\n",
    "            \n",
    "            shapes = [(geom, 1) for geom in gdf.geometry if geom is not None]\n",
    "\n",
    "            # Rasterize using reference metadata (shape and transform)\n",
    "            gt_mask = rasterize(shapes=shapes, out_shape=src1.shape, \n",
    "                                transform=src1.transform, fill=0, dtype=\"uint8\")\n",
    "            gt_flat = gt_mask.ravel()  # efficient way (no copy) to flatten the 2D mask (H x W) into a 1D vector\n",
    "            \n",
    "            # Clean NaNs\n",
    "            valid_mask = ~np.isnan(diff_flat).any(axis=1)\n",
    "\n",
    "            # Collect results\n",
    "            all_pixels_list.append(diff_flat[valid_mask])\n",
    "            all_labels_list.append(gt_flat[valid_mask])\n",
    "            \n",
    "            print(f\"Pair {i+1}/{len(list_raster_pairs)} processed: {np.sum(valid_mask)} valid pixels.\")\n",
    "\n",
    "    # 2) Global Concatenation and Scaling (bands might have different scales/units, shift expected value to 0 and standard deviation to 1)\n",
    "    X_global = np.vstack(all_pixels_list) # stacks the arrays vertically (TotalPixels, Bands)\n",
    "    y_global = np.concatenate(all_labels_list)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_global)\n",
    "    # operating in two steps \n",
    "    # fit goes through each column and calculates its expected value and standard deviation\n",
    "    # transforms each pixel with the formula stored in scaler  \n",
    "\n",
    "    # 3) PCA Execution\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    # first looks for eigenvectors containing maximum variance, then projects pixels on these axes\n",
    "    exp_var = pca.explained_variance_ratio_\n",
    "\n",
    "    # 4) Calculate centroids (mean position of each class) and separability\n",
    "    # Mean of PC1 and PC2 for 'No Change' (0) and 'Deforestation' (1)\n",
    "    centroid_no_change = np.mean(X_pca[y_global == 0], axis=0)\n",
    "    centroid_change = np.mean(X_pca[y_global == 1], axis=0)\n",
    "\n",
    "    dist_centroids = np.linalg.norm(centroid_change - centroid_no_change)\n",
    "    \n",
    "    std_0 = np.mean(np.std(X_pca[y_global == 0], axis=0))\n",
    "    std_1 = np.mean(np.std(X_pca[y_global == 1], axis=0))\n",
    "    \n",
    "    # Separability Ratio formula: S = d / (sigma0 + sigma1)\n",
    "    separability_ratio = dist_centroids / (std_0 + std_1)\n",
    "    \n",
    "\n",
    "   # 5) Visualization: PC1 vs PC2\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot background points\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_global, \n",
    "                         cmap='coolwarm', s=1, alpha=0.3)\n",
    "    # X_pca[:, 0/1] takes all the pixels from PC1/PC2, \n",
    "    # c=y_global colors each point according to its value in y_global (0 or 1)\n",
    "    # s=1 size of points, alpha=0,5 transparency \n",
    "\n",
    "    # Plot Centroids with high visibility\n",
    "    plt.scatter(centroid_no_change[0], centroid_no_change[1], color='blue', \n",
    "                marker='X', s=200, edgecolors='white', label='Mean: No Change')\n",
    "    plt.scatter(centroid_change[0], centroid_change[1], color='red', \n",
    "                marker='X', s=200, edgecolors='white', label='Mean: Deforestation')\n",
    "\n",
    "    plt.xlabel(f'PC1 ({exp_var[0]:.2%})')\n",
    "    plt.ylabel(f'PC2 ({exp_var[1]:.2%})')\n",
    "    plt.title('Global PCA Space with Class Centroids')\n",
    "    plt.legend(loc='lower right', markerscale=1)  # si we let plt.legend(), by default (loc= 'best'), it calculates the location of all pixels to know where there's some space, which can be long\n",
    "    plt.colorbar(scatter, label='Ground Truth (0: Stable, 1: Change)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    stats_text = f'Separability Ratio: {separability_ratio:.2f}'\n",
    "    plt.gca().text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, \n",
    "                   fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Separation distance in PCA space: {dist_centroids:.4f}\")\n",
    "    print(f\"Separability Ratio (S): {separability_ratio:.4f}\") \n",
    "\n",
    "    # 6) Correlation Circle\n",
    "    # Calculate the matrix of the coordinates of the variables (Corr(X_i, PC_j) = Component_ij * sqrt(eigenvalue_j))\n",
    "    \n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)  # pca.components_.T (n_features, n_components)\n",
    "\n",
    "    # Define variable names \n",
    "    feature_names = labels if labels is not None else [f'Band {i+1}' for i in range(X_global.shape[1])]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # Plot the circle\n",
    "    circle = plt.Circle((0, 0), 1, color='blue', fill=False, linestyle='--')\n",
    "    plt.gca().add_artist(circle)\n",
    "\n",
    "    # Plot arrows for each variable\n",
    "    for i in range(len(feature_names)):\n",
    "        plt.arrow(0, 0, loadings[i, 0], loadings[i, 1], \n",
    "                  color='red', alpha=0.8, head_width=0.05, length_includes_head=True)\n",
    "        plt.text(loadings[i, 0] * 1.15, loadings[i, 1] * 1.15, \n",
    "                 feature_names[i], color='black', ha='center', va='center', fontweight='bold')\n",
    "\n",
    "    # Formatting the circle plot\n",
    "    plt.xlim(-1.1, 1.1)\n",
    "    plt.ylim(-1.1, 1.1)\n",
    "    plt.axhline(0, color='black', linewidth=1)\n",
    "    plt.axvline(0, color='black', linewidth=1)\n",
    "    plt.xlabel(f'PC1 ({exp_var[0]:.2%})')\n",
    "    plt.ylabel(f'PC2 ({exp_var[1]:.2%})')\n",
    "    plt.title('Correlation Circle (Variable Importance)')\n",
    "    plt.grid(True, linestyle=':', alpha=0.6)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.show()\n",
    "\n",
    "    return pca, X_pca, y_global\n",
    "\n",
    "# Example \n",
    "labels = ['G0', 'contrast', 'dissimilarity', 'homogeneity', 'correlation', 'energy', 'ASM', 'variance', 'entropy']\n",
    "shp_path = r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_grd_truth\\zta1_grd_truth.shp\"\n",
    "list_raster_pairs = [(r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_explore\\zta1_pre_indices.tif\", r\"C:\\Users\\gbonlieu\\Documents\\herramienta\\change_detection_tool\\data\\preprocessed\\zta1\\zta1_explore\\zta1_post_indices.tif\")]\n",
    "pca_model, pca_data, labels = perform_global_pca_analysis(list_raster_pairs, [shp_path])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vigisar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
